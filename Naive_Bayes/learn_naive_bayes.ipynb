{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWord2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print \"the word: %s is not in my vocabulary!\" % word\n",
    "    return returnVec\n",
    "\n",
    "# 将setOfWord2Vec 替换为 bagOfWords2VecMN才是完整的多项式模型\n",
    "# 或者如《机器学习实战》中说的由词集模型，变为词袋模型，词集模型是介于伯努利模型与多项式模型之间的半吊子\n",
    "# 词集模型中，分子用的是每一类中出现某个单词的df，分母却是该类所有单词的数量\n",
    "# \n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)      #change to ones()  laplace smooth\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0                        #change to 2.0  laplace smooth\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = np.log(p0Num/p0Denom)          #change to log()\n",
    "    return p0Vect,p1Vect,pAbusive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainMatrix = np.array(trainMat)\n",
    "trainCategory = np.array(listClasses)\n",
    "numTrainDocs = len(trainMatrix)\n",
    "numWords = len(trainMatrix[0])\n",
    "pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "p0Num = np.ones(numWords)\n",
    "p1Num = np.ones(numWords)      #change to ones()  laplace smooth\n",
    "p0Denom = 2.0\n",
    "p1Denom = 2.0\n",
    "for i in range(numTrainDocs):\n",
    "    if trainCategory[i] == 1:\n",
    "        p1Num += trainMatrix[i]\n",
    "        p1Denom += sum(trainMatrix[i])\n",
    "    else:\n",
    "        p0Num += trainMatrix[i]\n",
    "        p0Denom += sum(trainMatrix[i])\n",
    "p1Vect = np.log(p1Num/p1Denom)          #change to log()\n",
    "p0Vect = np.log(p0Num/p0Denom)          #change to log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 3, 1, 1, 2, 1, 2, 2, 1,\n",
       "       2, 2, 2, 1, 2, 1, 2, 2, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMatrix[0,:] + trainMatrix[2,:] + trainMatrix[4,:] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  2.,  1.,  1.,  2.,  2.,  2.,  1.,  2.,  2.,  2.,  2.,\n",
       "        1.,  1.,  3.,  1.,  1.,  2.,  1.,  2.,  2.,  1.,  2.,  2.,  2.,\n",
       "        1.,  2.,  1.,  2.,  2.,  4.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(bagOfWords2VecMN(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is',\n",
       "       'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him',\n",
       "       'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe',\n",
       "       'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my'], \n",
       "      dtype='|S9')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(trainMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as :  0\n",
      "['stupid', 'garbage'] classified as :  1\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    #print vec2Classify.shape, p1Vec.shape\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)  # 对于文档中重复出现的词，在训练计算条件概率时，利用的是多项式模型，\n",
    "                                                      # 但是词向量又利用的是伯努利的模型，艹\n",
    "                                                      # 但是在测试时，测试样本重复出现的词，只算了一次\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWord2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    \n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWord2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "    \n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWord2Vec(myVocabList, testEntry))\n",
    "    print testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb)\n",
    "\n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\email\\\\spam\\\\1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-05619d5ce3f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'the error rate is : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrorCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mspamTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-05619d5ce3f9>\u001b[0m in \u001b[0;36mspamTest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfullText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m26\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mwordList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.\\\\email\\\\spam\\\\%d.txt'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mdocList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mfullText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '.\\\\email\\\\spam\\\\1.txt'"
     ]
    }
   ],
   "source": [
    "# 应用 ：垃圾邮件分类\n",
    "def textParse(bigString):\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) >2]\n",
    "\n",
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('.\\\\email\\\\spam\\\\%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        \n",
    "        wordList = textParse(open('.\\\\email\\\\ham\\\\%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "        \n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50)\n",
    "    testSet = []\n",
    "    for i in range(10):\n",
    "        randIndex = int( np.random.uniform(0, len(trainingSet)) )\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del trainingSet[randIndex]\n",
    "    \n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWord2Vec(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    \n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    #print trainingSet, p0V, p1V\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWord2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(wordVector, p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print 'the error rate is : ', float(errorCount)/len(testSet)\n",
    "        \n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postingList, classVec = loadDataSet()\n",
    "vocabSet = createVocabList(postingList)\n",
    "train_word2vec = []\n",
    "for post in postingList:\n",
    "    train_word2vec.append(setOfWord2Vec(vocabSet, post))\n",
    "p0Vect, p1Vect, pAbusive = trainNB0(train_word2vec, classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainMatrix = train_word2vec\n",
    "trainCategory = classVec\n",
    "numTrainDocs = len(trainMatrix)\n",
    "numWords = len(trainMatrix[0])\n",
    "pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "p0Num = np.zeros(numWords)\n",
    "p1Num = np.zeros(numWords)      #change to ones() \n",
    "p0Denom = 0.0\n",
    "p1Denom = 0.0  \n",
    "for i in range(numTrainDocs):\n",
    "    if trainCategory[i] == 1:\n",
    "        p1Num += trainMatrix[i]\n",
    "        p1Denom += sum(trainMatrix[i])\n",
    "    else:\n",
    "        p0Num += trainMatrix[i]\n",
    "        p0Denom += sum(trainMatrix[i])\n",
    "p1Vect = (p1Num/p1Denom)          #change to log()\n",
    "p0Vect = (p0Num/p0Denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print vocabSet,len(vocabSet)\n",
    "print trainMatrix[0]\n",
    "print trainMatrix[2]\n",
    "print trainMatrix[4]\n",
    "print p0Num.astype(int).tolist()\n",
    "print '\\n'\n",
    "print trainMatrix[1]\n",
    "print trainMatrix[3]\n",
    "print trainMatrix[5]\n",
    "print p1Num.astype(int).tolist()\n",
    "\n",
    "print sum(trainMatrix[0]), sum(trainMatrix[2]), sum(trainMatrix[4]), p0Denom\n",
    "print sum(trainMatrix[1]), sum(trainMatrix[3]), sum(trainMatrix[5]), p1Denom\n",
    "print p0Vect,sum(p0Vect)\n",
    "print p1Vect,sum(p1Vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 伯努利模型\n",
    "def my_trainNB(trainMatrix, trainCategory):\n",
    "    num_samples = len(trainMatrix)\n",
    "    num_features = len(trainMatrix[0])\n",
    "    num_values_per_fea = 2  #分别为0 和 1\n",
    "    num_class = 2 # len(set(trainCategory))\n",
    "    num_pos = sum(trainCategory)\n",
    "    num_neg = num_samples - num_pos\n",
    "    p_pos = num_pos / float(num_samples) #正样本频率，即是侮辱性语言的频率\n",
    "    p_neg = 1 - p_pos\n",
    "    num_pos_fea_1 = np.zeros(num_features)\n",
    "    num_neg_fea_1 = np.zeros(num_features)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        if trainCategory[i] == 1:\n",
    "            num_pos_fea_1 += trainMatrix[i]\n",
    "        elif trainCategory[i] == 0:\n",
    "            num_neg_fea_1 += trainMatrix[i]\n",
    "    p_pos_fea_1 = p1Num / num_pos  #在类别为正的时候，某个特征取值为1的概率，\n",
    "    p_pos_fea_0 = np.ones(num_features) - p_pos_fea_1 #在类别为正的时候，某个特征取值为0的概率\n",
    "    p_neg_fea_1 = p0Num / num_neg  #在类别为负的时候，某个特征取值为1的概率，\n",
    "    p_neg_fea_0 = np.ones(num_features) - p_neg_fea_1 #在类别为负的时候，某个特征取值为0的概率\n",
    "    print num_pos, num_neg, p_pos, p_neg\n",
    "    \n",
    "    print trainMatrix[1]\n",
    "    print trainMatrix[3]\n",
    "    print trainMatrix[5]\n",
    "    print num_pos_fea_1.astype(int).tolist()\n",
    "    print p_pos_fea_1\n",
    "    print p_pos_fea_0\n",
    "    print '\\n'\n",
    "    print trainMatrix[0]\n",
    "    print trainMatrix[2]\n",
    "    print trainMatrix[4]\n",
    "    print num_neg_fea_1.astype(int).tolist()\n",
    "    print p_neg_fea_1\n",
    "    print p_neg_fea_0\n",
    "            \n",
    "    \n",
    "my_trainNB(trainMatrix, trainCategory)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
